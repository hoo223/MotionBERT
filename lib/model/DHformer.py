import torch
import torch.nn as nn
import math
import warnings
import random
import numpy as np
from collections import OrderedDict
from functools import partial
from itertools import repeat
from lib.model.drop import DropPath

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

class MLP(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features) # 256 -> 256x4
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features) # 256x4 -> 256
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x

class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., st_mode='vanilla'):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads # 256//8 = 32
        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights
        self.scale = qk_scale or head_dim ** -0.5 # sqrt(32)

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim) # 256 -> 256
        self.mode = st_mode
        if self.mode == 'parallel':
            self.ts_attn = nn.Linear(dim*2, dim*2)
            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) 
        else: # spatial, temporal
            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) # 256 -> 256 x 3
        self.proj_drop = nn.Dropout(proj_drop)

        self.attn_count_s = None
        self.attn_count_t = None

    def forward(self, x, seqlen=1): # x=(BxF, 17, 256), seqlen=243
        B, N, C = x.shape # (BxF, 17, 256)
        
        if self.mode == 'series':
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_spatial(q, k, v)
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_temporal(q, k, v, seqlen=seqlen)
        elif self.mode == 'parallel':
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)
            x_t = self.forward_temporal(q, k, v, seqlen=seqlen)
            x_s = self.forward_spatial(q, k, v)
            
            alpha = torch.cat([x_s, x_t], dim=-1)
            alpha = alpha.mean(dim=1, keepdim=True)
            alpha = self.ts_attn(alpha).reshape(B, 1, C, 2)
            alpha = alpha.softmax(dim=-1)
            x = x_t * alpha[:,:,:,1] + x_s * alpha[:,:,:,0]
        elif self.mode == 'coupling':
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_coupling(q, k, v, seqlen=seqlen)
        elif self.mode == 'vanilla':
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_spatial(q, k, v)
        elif self.mode == 'temporal':
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) # x: (BxF, 17, 256) -> output: (BxF, 17, 256x3) -> reshape: (BxF, 17, 3, 8, 32) -> permute: (3, BxF, 8, 17, 32)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple) # q, k, v: (BxF, 8, 17, 32)
            x = self.forward_temporal(q, k, v, seqlen=seqlen)
        elif self.mode == 'spatial':
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) # x: (BxF, 17, 256) -> output: (BxF, 17, 256x3) -> reshape: (BxF, 17, 3, 8, 32) -> permute: (3, BxF, 8, 17, 32)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple) # q, k, v: (BxF, 8, 17, 32)
            x = self.forward_spatial(q, k, v) # (BxF, 17, 256)
        else:
            raise NotImplementedError(self.mode)
        x = self.proj(x) # (BxF, 17, 256) -> (BxF, 17, 256)
        x = self.proj_drop(x)
        return x
    
    def reshape_T(self, x, seqlen=1, inverse=False):
        if not inverse:
            N, C = x.shape[-2:]
            x = x.reshape(-1, seqlen, self.num_heads, N, C).transpose(1,2)
            x = x.reshape(-1, self.num_heads, seqlen*N, C) #(B, H, TN, c)
        else:
            TN, C = x.shape[-2:]
            x = x.reshape(-1, self.num_heads, seqlen, TN // seqlen, C).transpose(1,2)
            x = x.reshape(-1, self.num_heads, TN // seqlen, C) #(BT, H, N, C)
        return x 

    def forward_coupling(self, q, k, v, seqlen=8):
        BT, _, N, C = q.shape
        q = self.reshape_T(q, seqlen)
        k = self.reshape_T(k, seqlen)
        v = self.reshape_T(v, seqlen)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = attn @ v
        x = self.reshape_T(x, seqlen, inverse=True)
        x = x.transpose(1,2).reshape(BT, N, C*self.num_heads)
        return x

    def forward_spatial(self, q, k, v): # q, k, v: (BxF, 8, 17, 32)
        B, _, N, C = q.shape
        # scaled dot-product attention
        attn = (q @ k.transpose(-2, -1)) * self.scale # (BxF, 8, 17, 32) @ (BxF, 8, 32, 17) * (1) = (BxF, 8, 17, 17)
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = attn @ v # BxF, 8, 17, 17) @ (BxF, 8, 17, 32) = (BxF, 8, 17, 32)
        x = x.transpose(1,2).reshape(B, N, C*self.num_heads) # x: (BxF, 8, 17, 32) -> transpose: (BxF, 17, 8, 32) -> reshape: (BxF, 17, 32x8) = (BxF, 17, 256)
        return x
        
    def forward_temporal(self, q, k, v, seqlen=8): # q, k, v: (BxF, 8, 17, 32), seqlen=243
        B, _, N, C = q.shape
        qt = q.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4) # (BxF, 8, 17, 32) -> reshape: (B, 243, 8, 17, 32) -> permute: (B, 8, 17, 243, 32) # (B, H, N, T, C) 
        kt = k.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4) # (BxF, 8, 17, 32) -> reshape: (B, 243, 8, 17, 32) -> permute: (B, 8, 17, 243, 32) # (B, H, N, T, C)
        vt = v.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4) # (BxF, 8, 17, 32) -> reshape: (B, 243, 8, 17, 32) -> permute: (B, 8, 17, 243, 32) # (B, H, N, T, C)

        attn = (qt @ kt.transpose(-2, -1)) * self.scale # (B, 8, 17, 243, 32) @ (B, 8, 17, 32, 243) * (1) = (B, 8, 17, 243, 243)
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = attn @ vt # (B, 8, 17, 243, 243) @ (B, 8, 17, 243, 32) = (B, 8, 17, 243, 32) # (B, H, N, T, C)
        x = x.permute(0, 3, 2, 1, 4).reshape(B, N, C*self.num_heads) # x:(B, 8, 17, 243, 32) -> permute: (B, 243, 17, 8, 32) -> reshape: (BxF, 17, 256)
        return x

    def count_attn(self, attn):
        attn = attn.detach().cpu().numpy()
        attn = attn.mean(axis=1)
        attn_t = attn[:, :, 1].mean(axis=1)
        attn_s = attn[:, :, 0].mean(axis=1)
        if self.attn_count_s is None:
            self.attn_count_s = attn_s
            self.attn_count_t = attn_t
        else:
            self.attn_count_s = np.concatenate([self.attn_count_s, attn_s], axis=0)
            self.attn_count_t = np.concatenate([self.attn_count_t, attn_t], axis=0)

class Block(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., mlp_out_ratio=1., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, att_fuse=False):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.norm2 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
        
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity() 
        # DropPath: 확률적으로 skip connection(x+f(x)에서 f(x))만 남기는 regularization 방법 
        # nn.Identity(): 입력값을 그대로 출력하는 layer
        mlp_hidden_dim = int(dim * mlp_ratio) # 256 x 4
        mlp_out_dim = int(dim * mlp_out_ratio) # 256
        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, out_features=mlp_out_dim, act_layer=act_layer, drop=drop)
    def forward(self, x, seqlen=1): # x=(BxF, 17, 256), seqlen=243
        x = x + self.drop_path(self.attn(self.norm1(x), seqlen)) # (BxF, 17, 256)
        x = x + self.drop_path(self.mlp(self.norm2(x))) # (BxF, 17, 256)
        return x

class DHformer(nn.Module):
    def __init__(self, dim_in=2, dim_out=3, dim_feat=256, dim_rep=256,
                 depth=5, num_heads=8, mlp_ratio=4, 
                 num_joints=17, maxlen=243, 
                 qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim_out = dim_out
        self.dim_feat = dim_feat
        self.joints_embed = nn.Linear(dim_in, dim_feat) # FC layer before DSTformer - (BxF, 17, 3) -> (BxF, 17, 256)
        self.pos_drop = nn.Dropout(p=drop_rate) 
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        # attention block
        self.blocks = nn.ModuleList([
            Block(
                dim=dim_feat, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)
            for i in range(depth)])
        self.norm = norm_layer(dim_feat)
        if dim_rep:
            self.pre_logits = nn.Sequential(OrderedDict([ # final pre-FC layer
                ('fc', nn.Linear(dim_feat, dim_rep)),
                ('act', nn.Tanh())
            ]))
        else:
            self.pre_logits = nn.Identity()
        self.head = nn.Linear(dim_rep, dim_out) if dim_out > 0 else nn.Identity() # final FC layer 
        self.temp_embed = nn.Parameter(torch.zeros(1, maxlen, 1, dim_feat)) # learnable temporal positional encoding - (1, 243, 1, 256)
        self.pos_embed = nn.Parameter(torch.zeros(1, num_joints, dim_feat)) # learnable spatial positional encoding - (1, 17, 256)
        trunc_normal_(self.temp_embed, std=.02)
        trunc_normal_(self.pos_embed, std=.02)
        self.apply(self._init_weights) # 해당 Module의 모든 sub-module에 인수받은 함수를 적용시켜준다.

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def get_classifier(self):
        return self.head

    def reset_classifier(self, dim_out, global_pool=''):
        self.dim_out = dim_out
        self.head = nn.Linear(self.dim_feat, dim_out) if dim_out > 0 else nn.Identity()

    def forward(self, x, return_rep=False):   
        B, F, J, C = x.shape # Batch, Frame, Joint, Channel
        x = x.reshape(-1, J, C) # Batch*Frame, Joint, Channel
        BF = x.shape[0]
        x = self.joints_embed(x) # feature embedding - (BxF, 17, 3) -> (BxF, 17, 256)
        x = x + self.pos_embed # add learnable spatial positional encoding - (BxF, 17, 256) + (1, 17, 256)
        _, J, C = x.shape # J=17, C=256
        x = x.reshape(-1, F, J, C) + self.temp_embed[:,:F,:,:] # add learnable temporal positional encoding
        x = x.reshape(BF, J, C) # (BxF, 17, 256)
        x = self.pos_drop(x)
        #alphas = []
        for idx, blk in enumerate(zip(self.blocks)):
            x = blk(x, F) # Block의 forward 함수 -> x: (BxF, 17, 256), F=243, x_st: (BxF, 17, 256)
        x = self.norm(x) # layer normalization
        x = x.reshape(B, F, J, -1) # # (B, F, 17, 256)
        x = self.pre_logits(x)  # [B, F, J, dim_feat] -> motion representation E or pass
        if return_rep:
            return x
        x = self.head(x) # 3D motion X_hat # (N, F, 17, dim_rep) -> (N, F, 17, 3)
        return x

    def get_representation(self, x):
        return self.forward(x, return_rep=True)