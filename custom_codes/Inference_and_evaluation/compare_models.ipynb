{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Skeleton Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import interact\n",
    "import copy\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from test_utils import *\n",
    "\n",
    "from test_utils import get_rootrel_pose, _weak_project, camera_to_image_frame, _infer_box, optimize_scaling_factor\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "plt.switch_backend('TkAgg')\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "motionbert_root = \"/home/hrai/codes/MotionBERT\"\n",
    "mb_output_root = motionbert_root + \"/output\"\n",
    "mb_aihub_output_root = mb_output_root + \"/aihub\"\n",
    "assert os.path.exists(mb_aihub_output_root) == True # check if the path exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/hrai/codes/MotionBERT/output/aihub/MB_ft_h36m_30_M160A_3',\n",
       " '/home/hrai/codes/MotionBERT/output/aihub/FT_MB_ft_h36m_MB_ft_aihub_sport_243_dazzling-shape-17_30_M160A_3')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_path = mb_aihub_output_root + \"/MB_ft_h36m_30_M160A_3\"\n",
    "model2_path = mb_aihub_output_root + \"/FT_MB_ft_h36m_MB_ft_aihub_sport_243_dazzling-shape-17_30_M160A_3\"\n",
    "model1_path, model2_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model1 output\n",
    "mp4, npy = os.listdir(model1_path)\n",
    "assert 'mp4' in mp4 # check if mp4 file exists\n",
    "assert 'npy' in npy # check if npy file exists\n",
    "output = np.load(os.path.join(model1_path, npy))\n",
    "model1_pred = output[frame_num]\n",
    "model1_pred_hat = get_rootrel_pose(model1_pred)\n",
    "\n",
    "# load model2 output\n",
    "mp4, npy = os.listdir(model2_path)\n",
    "assert 'mp4' in mp4 # check if mp4 file exists\n",
    "assert 'npy' in npy # check if npy file exists\n",
    "output = np.load(os.path.join(model2_path, npy))\n",
    "model2_pred = output[frame_num]\n",
    "model2_pred_hat = get_rootrel_pose(model2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(0, figsize=(10, 10))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.set_xlim(-512, 512)\n",
    "ax.set_ylim(-512, 512)\n",
    "ax.set_zlim(-512, 512)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.view_init(elev=12., azim=80)\n",
    "#show3Dpose(model1_pred_hat*1000, ax)\n",
    "show3Dpose(model2_pred_hat*1000, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hrai/Datasets/HAAI/AIHUB/label/train/[라벨]3D_json.zip'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aihub_root = \"/home/hrai/Datasets/HAAI/AIHUB\"\n",
    "aihub_3d_gt_path = os.path.join(aihub_root, \"label/train/[라벨]3D_json.zip\")\n",
    "aihub_3d_gt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load zip file\n",
    "zip_label = ZipFile(os.path.join(aihub_3d_gt_path), 'r')\n",
    "list_input = zip_label.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = '30'\n",
    "actor = 'M160A'\n",
    "frame = '30'\n",
    "# actor = 'M180D' #'M160A'\n",
    "# frame = '311' # '30'\n",
    "load_path = \"3D_json/{}_{}/3D_{}_{}_{}.json\".format(action, actor, action, actor, frame)\n",
    "gt = json.loads(zip_label.read(load_path).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'annotations'])\n",
      "dict_keys(['supercategory', 'action_category_id', 'actor_id', '3d_pos', '3d_rot'])\n",
      "dict_keys(['frame_no', 'obj_path', '3d_pos', '3d_rot', 'trans_params'])\n"
     ]
    }
   ],
   "source": [
    "print(gt.keys())\n",
    "print(gt['info'].keys())\n",
    "print(gt['annotations'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load camera parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aihub_camera_param_path = os.path.join(aihub_root, \"label/train/Camera_json_train.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load zip file\n",
    "zip_param = ZipFile(os.path.join(aihub_camera_param_path), 'r')\n",
    "list_input = zip_param.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = '30'\n",
    "# #actor = 'M180D' \n",
    "# cam_num = '4'\n",
    "actor = 'M160A'\n",
    "cam_num = '3'\n",
    "load_path = \"Camera_json/train/{}_{}_{}.json\".format(action, actor, cam_num)\n",
    "camera_param = json.loads(zip_param.read(load_path).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'camera_date': '20201008',\n",
       " 'camera_no': 3,\n",
       " 'extrinsics': [[-0.733395875, 0.026952436, 0.679267645, -8.36987305],\n",
       "  [-0.155711249, -0.979308605, -0.129261598, 740.389099],\n",
       "  [0.66172874, -0.200569466, 0.722417653, 4432.7334]],\n",
       " 'intrinsics': [[0.68166077, 0.0, 0.50988585],\n",
       "  [0.0, 0.68166077, 0.26416245],\n",
       "  [0.0, 0.0, 1.0]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 1920\n",
    "H = 1080\n",
    "extrinsic_properties = np.array(camera_param['extrinsics'])\n",
    "R = copy.deepcopy(np.array(camera_param['extrinsics'])[:,:3])\n",
    "T = copy.deepcopy(np.array(camera_param['extrinsics'])[:,3])\n",
    "R_c = R.T\n",
    "C = - np.matmul(R_c, T)\n",
    "intrinsic_properties = np.array(camera_param['intrinsics']) # normalized intrinsic matrix\n",
    "intrinsic_properties[:2, :] *= W # denormalize\n",
    "fx = intrinsic_properties[0,0]\n",
    "fy = intrinsic_properties[1,1]\n",
    "cx = intrinsic_properties[0,2]\n",
    "cy = intrinsic_properties[1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aihub to h36m pose\n",
    "world_3d = aihub2h36m(np.array(gt['annotations']['3d_pos'])[:, :3].reshape(1, 24, 3))[0]\n",
    "world_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(0, figsize=(10, 10))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "min_, max_ = -2000, 2000\n",
    "ax.set_xlim(min_, max_)\n",
    "ax.set_ylim(min_, max_)\n",
    "ax.set_zlim(min_, max_)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.view_init(elev=12., azim=80)\n",
    "show3Dpose(world_3d, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# world to camera\n",
    "pos = copy.deepcopy(world_3d)\n",
    "cam_3d = World2CameraCoordinate(pos, extrinsic_properties) # World coordinate -> Camera coordinate\n",
    "cam_3d_hat = get_rootrel_pose(cam_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(0, figsize=(10, 10))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "min_, max_ = -2000, 2000\n",
    "ax.set_xlim(min_, max_)\n",
    "ax.set_ylim(min_, max_)\n",
    "ax.set_zlim(min_, max_)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.view_init(elev=12., azim=80)\n",
    "show3Dpose(cam_3d_hat, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camera to image\n",
    "box = _infer_box(cam_3d, {'fx': fx, 'fy': fy, 'cx': cx, 'cy': cy}, 0)\n",
    "img_2d, img_3d = camera_to_image_frame(cam_3d, box, {'fx': fx, 'fy': fy, 'cx': cx, 'cy': cy}, 0) \n",
    "img_2d_hat = get_rootrel_pose(img_2d) # (17, 2) # root-relative pose \n",
    "img_3d_hat = get_rootrel_pose(img_3d) # (17, 3) # root-relative pose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Vegetebird/MHFormer/blob/main/demo/vis.py\n",
    "def get_2d_pose_image(kps, img, box=None):\n",
    "    connections = [[0, 1], [1, 2], [2, 3], [0, 4], [4, 5],\n",
    "                   [5, 6], [0, 7], [7, 8], [8, 9], [9, 10],\n",
    "                   [8, 11], [11, 12], [12, 13], [8, 14], [14, 15], [15, 16]]\n",
    "\n",
    "    LR = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], dtype=bool)\n",
    "\n",
    "    lcolor = (255, 0, 0)\n",
    "    rcolor = (0, 0, 255)\n",
    "    thickness = 3\n",
    "\n",
    "    for j,c in enumerate(connections):\n",
    "        start = map(int, kps[c[0]])\n",
    "        end = map(int, kps[c[1]])\n",
    "        start = list(start)\n",
    "        end = list(end)\n",
    "        cv2.line(img, (start[0], start[1]), (end[0], end[1]), lcolor if LR[j] else rcolor, thickness)\n",
    "        cv2.circle(img, (start[0], start[1]), thickness=-1, color=(0, 255, 0), radius=3)\n",
    "        cv2.circle(img, (end[0], end[1]), thickness=-1, color=(0, 255, 0), radius=3)\n",
    "        if box is not None:\n",
    "            box = box.astype(int)\n",
    "            cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "\n",
    "    return img\n",
    "\n",
    "def plot_cv2_image(img):\n",
    "    #plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize 2D pose\n",
    "img_path = \"/home/hrai/Datasets/HAAI/AIHUB/30_M160A_3/30_M160A_3_30.jpg\"\n",
    "#img_path = \"/home/hrai/Datasets/HAAI/AIHUB/30_M180D_4/30_M180D_4_311.jpg\"\n",
    "#img = np.ones([1080, 1920, 3])\n",
    "img = cv2.imread(img_path)\n",
    "img = get_2d_pose_image(img_2d, img, box)\n",
    "plot_cv2_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "# img_2d\n",
    "img = np.ones([2000, 2000, 3])\n",
    "img = get_2d_pose_image(cam_3d_hat[:, :2] + 1000, img)\n",
    "plot_cv2_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3579845428466797"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimize scaling factor\n",
    "pred_lambda = optimize_scaling_factor(img_3d_hat, cam_3d_hat) # x,y,z 사용\n",
    "pred_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_25d = img_3d * pred_lambda\n",
    "img_25d_hat = get_rootrel_pose(img_25d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(0, figsize=(10, 10))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.set_xlim(-512, 512)\n",
    "ax.set_ylim(-512, 512)\n",
    "ax.set_zlim(-512, 512)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.view_init(elev=12., azim=80)\n",
    "\n",
    "show3Dpose(img_25d_hat, ax)\n",
    "show3Dpose(cam_3d_hat, ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize inference and gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_hat_scaled = pred_hat * pred_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(0, figsize=(10, 10))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.set_xlim(-512, 512)\n",
    "ax.set_ylim(-512, 512)\n",
    "ax.set_zlim(-512, 512)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.view_init(elev=12., azim=80)\n",
    "\n",
    "show3Dpose(img_25d_hat, ax)\n",
    "show3Dpose(pred_hat_scaled, ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate MPJPE for single pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from test_utils import MPJPE_for_single_pose, MPJPE_for_multiple_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.47767607231927 mm\n"
     ]
    }
   ],
   "source": [
    "mpjpe = MPJPE_for_single_pose(img_25d_hat, cam_3d_hat)\n",
    "print(mpjpe, \"mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.43126661780063 mm\n"
     ]
    }
   ],
   "source": [
    "mpjpe = MPJPE_for_single_pose(img_25d_hat, pred_hat_scaled)\n",
    "print(mpjpe, \"mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For multiple frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 M160A 3\n",
      "3D_json/30_M160A/3D_30_M160A_30.json\n"
     ]
    }
   ],
   "source": [
    "# whitelist\n",
    "action_list = ['30']\n",
    "actor_list = ['M160A']\n",
    "frame_list = ['30']\n",
    "\n",
    "#list_input = zip_label.namelist()\n",
    "for action in action_list:\n",
    "    for actor in actor_list:\n",
    "        for frame in frame_list:\n",
    "            print(action, actor, cam_num)\n",
    "            load_path = \"3D_json/{}_{}/3D_{}_{}_{}.json\".format(action, actor, action, actor, frame)\n",
    "            print(load_path)\n",
    "            data_label = json.loads(zip_label.read(load_path).decode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motionbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "996031ba2a0f3c1298a339c0299835a7fe1ef636d9e79358bc474ca43ed2ac18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
