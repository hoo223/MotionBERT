{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python train.py \\\n",
    "# --config configs/pose3d/MB_train_h36m.yaml \\\n",
    "# --evaluate checkpoint/pose3d/MB_train_h36m/best_epoch.bin         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "user = getpass.getuser()\n",
    "motionbert_root = '/home/{}/codes/MotionBERT'.format(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import errno\n",
    "import math\n",
    "import pickle\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import copy\n",
    "import random\n",
    "import prettytable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.chdir(motionbert_root)\n",
    "\n",
    "from lib.utils.tools import *\n",
    "from lib.utils.learning import *\n",
    "from lib.utils.utils_data import flip_data\n",
    "from lib.data.dataset_motion_2d import PoseTrackDataset2D, InstaVDataset2D\n",
    "from lib.data.dataset_motion_3d import MotionDataset3D\n",
    "from lib.data.augmentation import Augmenter2D\n",
    "from lib.data.datareader_aihub import DataReaderAIHUB\n",
    "from lib.model.loss import *\n",
    "\n",
    "from train import set_random_seed, save_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = 'MB_ft_tr_aihub_sport_ts_30'\n",
    "model_name = 'FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30'\n",
    "#model_name = 'MB_train_h36m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "opts = easydict.EasyDict({\n",
    "    \"config\": \"configs/pose3d/{}.yaml\".format(config),\n",
    "    \"checkpoint\": 'checkpoint',\n",
    "    \"pretrained\": 'checkpoint',\n",
    "    \"resume\": '',\n",
    "    \"evaluate\": 'checkpoint/pose3d/{}/best_epoch.bin'.format(model_name),\n",
    "    \"selection\": 'best_epoch.bin',\n",
    "    \"seed\": 0,\n",
    "    })\n",
    "set_random_seed(opts.seed)\n",
    "args = get_config(opts.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(opts.checkpoint)\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise RuntimeError('Unable to create checkpoint directory:', opts.checkpoint)\n",
    "train_writer = tensorboardX.SummaryWriter(os.path.join(opts.checkpoint, \"logs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AIHUB_tr_SPORT_ts_30']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.subset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "print('Loading dataset...')\n",
    "trainloader_params = {\n",
    "        'batch_size': args.batch_size,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 12,\n",
    "        'pin_memory': True,\n",
    "        'prefetch_factor': 4,\n",
    "        'persistent_workers': True\n",
    "}\n",
    "\n",
    "testloader_params = {\n",
    "        'batch_size': args.batch_size,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 12,\n",
    "        'pin_memory': True,\n",
    "        'prefetch_factor': 4,\n",
    "        'persistent_workers': True\n",
    "}\n",
    "\n",
    "train_dataset = MotionDataset3D(args, args.subset_list, 'train')\n",
    "test_dataset = MotionDataset3D(args, args.subset_list, 'test')\n",
    "train_loader_3d = DataLoader(train_dataset, **trainloader_params)\n",
    "test_loader = DataLoader(test_dataset, **testloader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datareader = DataReaderAIHUB(n_frames=args.clip_len, sample_stride=args.sample_stride, data_stride_train=args.data_stride, data_stride_test=args.clip_len, dt_root = 'data/motion3d', dt_file=args.dt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Trainable parameter count: 42466317\n"
     ]
    }
   ],
   "source": [
    "min_loss = 100000\n",
    "model_backbone = load_backbone(args)\n",
    "model_params = 0\n",
    "for parameter in model_backbone.parameters():\n",
    "    model_params = model_params + parameter.numel()\n",
    "print('INFO: Trainable parameter count:', model_params)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_backbone = nn.DataParallel(model_backbone)\n",
    "    model_backbone = model_backbone.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " '',\n",
       " 'checkpoint/pose3d/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30/best_epoch.bin')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.finetune, opts.resume, opts.evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/pose3d/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30/best_epoch.bin\n"
     ]
    }
   ],
   "source": [
    "chk_filename = opts.evaluate if opts.evaluate else opts.resume\n",
    "print('Loading checkpoint', chk_filename)\n",
    "checkpoint = torch.load(chk_filename, map_location=lambda storage, loc: storage)\n",
    "model_backbone.load_state_dict(checkpoint['model_pos'], strict=True)\n",
    "model_pos = model_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.partial_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoint/pose3d/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30/best_epoch.bin'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts.evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args, model_pos, test_loader, datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, True, False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.no_conf, args.flip, args.rootrel, args.gt_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.flip = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "results_all = []\n",
    "model_pos.eval()            \n",
    "with torch.no_grad():\n",
    "    for batch_input, batch_gt in tqdm(test_loader):\n",
    "        N, T = batch_gt.shape[:2] # B, N\n",
    "        if torch.cuda.is_available():\n",
    "            batch_input = batch_input.cuda()\n",
    "        if args.flip:    \n",
    "            batch_input_flip = flip_data(batch_input)\n",
    "            predicted_3d_pos_1 = model_pos(batch_input)\n",
    "            predicted_3d_pos_flip = model_pos(batch_input_flip)\n",
    "            predicted_3d_pos_2 = flip_data(predicted_3d_pos_flip)                   # Flip back\n",
    "            predicted_3d_pos = (predicted_3d_pos_1+predicted_3d_pos_2) / 2\n",
    "        else:\n",
    "            predicted_3d_pos = model_pos(batch_input)\n",
    "        results_all.append(predicted_3d_pos.cpu().numpy())\n",
    "results_all = np.concatenate(results_all)\n",
    "results_all = datareader.denormalize(results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 243, 17, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('custom_codes/evaluation/{}_result_denormalized.npy'.format(model_name), results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = np.load('custom_codes/evaluation/{}_result_denormalized.npy'.format(model_name))\n",
    "\n",
    "_, split_id_test = datareader.get_split_id()\n",
    "actions = np.array(datareader.dt_dataset['test']['action'])\n",
    "factors = np.array(datareader.dt_dataset['test']['2.5d_factor'])\n",
    "gts = np.array(datareader.dt_dataset['test']['joints_2.5d_image'])\n",
    "sources = np.array(datareader.dt_dataset['test']['source'])\n",
    "\n",
    "num_test_frames = len(actions)\n",
    "frames = np.array(range(num_test_frames))\n",
    "action_clips = np.array([actions[split_id_test[i]] for i in range(len(split_id_test))]) # actions[split_id_test]\n",
    "factor_clips = np.array([factors[split_id_test[i]] for i in range(len(split_id_test))]) # factors[split_id_test]\n",
    "source_clips = np.array([sources[split_id_test[i]] for i in range(len(split_id_test))]) # sources[split_id_test]\n",
    "frame_clips = np.array([frames[split_id_test[i]] for i in range(len(split_id_test))]) # frames[split_id_test]\n",
    "gt_clips = np.array([gts[split_id_test[i]] for i in range(len(split_id_test))]) # gts[split_id_test]\n",
    "assert len(results_all)==len(action_clips)\n",
    "\n",
    "e1_all = np.zeros(num_test_frames)\n",
    "e2_all = np.zeros(num_test_frames)\n",
    "oc = np.zeros(num_test_frames)\n",
    "action_names = sorted(set(datareader.dt_dataset['test']['action']))\n",
    "block_list = ['s_09_act_05_subact_02', \n",
    "                's_09_act_10_subact_02', \n",
    "                's_09_act_13_subact_01']\n",
    "\n",
    "for idx in range(len(action_clips)):\n",
    "    source = source_clips[idx][0]\n",
    "    if source in block_list:\n",
    "        continue\n",
    "    frame_list = frame_clips[idx] # numpy.ndarray\n",
    "    action = action_clips[idx][0]\n",
    "    factor = factor_clips[idx][:,None,None]\n",
    "    gt = gt_clips[idx]\n",
    "    pred = copy.deepcopy(results_all[idx])\n",
    "    pred *= factor\n",
    "    \n",
    "    # Root-relative Errors\n",
    "    pred = pred - pred[:,0:1,:] # (243, 17, 3)\n",
    "    gt = gt - gt[:,0:1,:] # (243, 17, 3)\n",
    "    err1 = mpjpe(pred, gt) # (243,)\n",
    "    err2 = p_mpjpe(pred, gt) # (243,)\n",
    "    e1_all[frame_list] += err1 # numpy.ndarray를 인덱스로 사용 가능\n",
    "    e2_all[frame_list] += err2\n",
    "    oc[frame_list] += 1 # 프레임별 카운팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "| test_name |         30         |\n",
      "+-----------+--------------------+\n",
      "|     P1    | 67.40624741414243  |\n",
      "|     P2    | 46.555171046777424 |\n",
      "+-----------+--------------------+\n",
      "Protocol #1 Error (MPJPE): 67.40624741414243 mm\n",
      "Protocol #2 Error (P-MPJPE): 46.555171046777424 mm\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "results_procrustes = {}\n",
    "\n",
    "for action in action_names:\n",
    "    results[action] = []\n",
    "    results_procrustes[action] = []\n",
    "\n",
    "for idx in range(num_test_frames):\n",
    "    if e1_all[idx] > 0:\n",
    "        err1 = e1_all[idx] / oc[idx]\n",
    "        err2 = e2_all[idx] / oc[idx]\n",
    "        action = actions[idx]\n",
    "        results[action].append(err1)\n",
    "        results_procrustes[action].append(err2)\n",
    "\n",
    "final_result = []\n",
    "final_result_procrustes = []\n",
    "summary_table = prettytable.PrettyTable()\n",
    "summary_table.field_names = ['test_name'] + action_names\n",
    "for action in action_names:\n",
    "    final_result.append(np.mean(results[action]))\n",
    "    final_result_procrustes.append(np.mean(results_procrustes[action]))\n",
    "summary_table.add_row(['P1'] + final_result)\n",
    "summary_table.add_row(['P2'] + final_result_procrustes)\n",
    "print(summary_table)\n",
    "e1 = np.mean(np.array(final_result))\n",
    "e2 = np.mean(np.array(final_result_procrustes))\n",
    "print('Protocol #1 Error (MPJPE):', e1, 'mm')\n",
    "print('Protocol #2 Error (P-MPJPE):', e2, 'mm')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "os.chdir(motionbert_root)\n",
    "from custom_codes.test_utils import *\n",
    "\n",
    "plt.switch_backend('TkAgg')\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((243, 17, 3), (243, 17, 3))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape, gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = 200\n",
    "visualize_3d_pose([pred[frame], gt[frame]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11, 243), (11, 243))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_clips.shape, frame_clips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('res_30_F170D_5', 3142)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_clips[-1][frame], frame_clips[-1][frame]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize one clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIHUB_tr_SPORT_ts_30/test/00000010.pkl\n",
    "idx = 10\n",
    "factor = factor_clips[idx][:,None,None]\n",
    "gt = copy.deepcopy(gt_clips[idx])\n",
    "pred = copy.deepcopy(results_all[idx])\n",
    "gt /= factor\n",
    "pred = pred - pred[:,0:1,:] # (243, 17, 3)\n",
    "gt = gt - gt[:,0:1,:] # (243, 17, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243/243 [00:59<00:00,  4.10it/s]\n"
     ]
    }
   ],
   "source": [
    "xlim=(-512, 512)\n",
    "ylim=(-512, 512)\n",
    "zlim=(-512, 512)\n",
    "fig = plt.figure(0, figsize=(10, 10))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_zlim(zlim)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "for i in tqdm(range(pred.shape[0])):\n",
    "    _ax = copy.deepcopy(ax)\n",
    "    _ax.view_init(elev=12., azim=80+i)\n",
    "    visualize_multiple_3d_pose([pred[i], gt[i]], _ax, save=True, save_path='./custom_codes/evaluation/{}_idx{}_result'.format(model_name, idx), name='{}.jpg'.format(i), i=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/0.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1000, 1000) to (1008, 1008) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/1.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/2.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/3.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/4.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/5.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/6.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/7.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/8.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/9.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/10.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/11.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/12.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/13.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/14.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/15.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/16.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/17.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/18.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/19.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/20.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/21.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/22.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/23.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/24.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/25.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/26.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/27.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/28.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/29.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/30.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/31.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/32.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/33.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/34.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/35.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/36.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/37.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/38.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/39.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/40.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/41.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/42.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/43.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/44.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/45.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/46.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/47.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/48.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/49.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/50.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/51.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/52.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/53.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/54.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/55.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/56.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/57.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/58.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/59.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/60.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/61.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/62.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/63.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/64.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/65.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/66.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/67.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/68.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/69.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/70.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/71.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/72.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/73.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/74.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/75.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/76.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/77.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/78.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/79.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/80.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/81.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/82.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/83.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/84.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/85.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/86.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/87.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/88.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/89.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/90.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/91.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/92.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/93.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/94.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/95.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/96.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/97.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/98.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/99.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/100.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/101.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/102.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/103.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/104.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/105.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/106.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/107.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/108.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/109.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/110.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/111.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/112.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/113.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/114.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/115.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/116.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/117.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/118.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/119.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/120.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/121.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/122.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/123.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/124.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/125.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/126.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/127.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/128.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/129.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/130.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/131.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/132.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/133.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/134.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/135.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/136.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/137.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/138.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/139.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/140.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/141.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/142.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/143.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/144.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/145.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/146.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/147.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/148.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/149.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/150.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/151.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/152.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/153.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/154.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/155.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/156.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/157.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/158.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/159.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/160.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/161.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/162.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/163.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/164.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/165.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/166.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/167.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/168.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/169.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/170.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/171.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/172.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/173.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/174.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/175.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/176.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/177.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/178.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/179.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/180.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/181.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/182.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/183.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/184.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/185.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/186.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/187.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/188.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/189.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/190.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/191.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/192.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/193.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/194.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/195.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/196.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/197.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/198.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/199.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/200.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/201.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/202.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/203.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/204.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/205.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/206.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/207.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/208.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/209.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/210.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/211.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/212.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/213.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/214.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/215.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/216.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/217.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/218.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/219.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/220.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/221.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/222.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/223.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/224.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/225.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/226.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/227.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/228.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/229.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/230.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/231.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/232.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/233.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/234.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/235.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/236.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/237.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/238.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/239.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/240.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/241.jpg\n",
      "./custom_codes/evaluation/FT-MB_ft_h36m-MB_ft_tr_aihub_sport_ts_30_idx10_result/242.jpg\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "from natsort import natsorted\n",
    "\n",
    "img_list = natsorted(os.listdir('./custom_codes/evaluation/{}_idx{}_result'.format(model_name, idx)))\n",
    "videowriter = imageio.get_writer('./custom_codes/evaluation/{}_idx{}_result/video.mp4'.format(model_name, idx), fps=30)\n",
    "\n",
    "for img in img_list:\n",
    "    img_path = os.path.join('./custom_codes/evaluation/{}_idx{}_result'.format(model_name, idx), img)\n",
    "    print(img_path)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGBA)\n",
    "    videowriter.append_data(img)\n",
    "videowriter.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D3DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install timm einops\n",
    "d3dp_root = \"/home/hrai/codes/D3DP\"\n",
    "os.chdir(d3dp_root)\n",
    "from common.diffusionpose import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def d3dp_parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Training script')\n",
    "\n",
    "    # General arguments\n",
    "    parser.add_argument('-d', '--dataset', default='h36m', type=str, metavar='NAME', help='target dataset') # h36m or humaneva\n",
    "    parser.add_argument('-k', '--keypoints', default='cpn_ft_h36m_dbb', type=str, metavar='NAME', help='2D detections to use')\n",
    "    parser.add_argument('-str', '--subjects-train', default='S1,S5,S6,S7,S8', type=str, metavar='LIST',\n",
    "                        help='training subjects separated by comma')\n",
    "    parser.add_argument('-ste', '--subjects-test', default='S9,S11', type=str, metavar='LIST', help='test subjects separated by comma')\n",
    "    parser.add_argument('-sun', '--subjects-unlabeled', default='', type=str, metavar='LIST',\n",
    "                        help='unlabeled subjects separated by comma for self-supervision')\n",
    "    parser.add_argument('-a', '--actions', default='*', type=str, metavar='LIST',\n",
    "                        help='actions to train/test on, separated by comma, or * for all')\n",
    "    parser.add_argument('-c', '--checkpoint', default='checkpoint', type=str, metavar='PATH',\n",
    "                        help='checkpoint directory')\n",
    "    parser.add_argument('-l', '--log', default='log/default', type=str, metavar='PATH',\n",
    "                        help='log file directory')\n",
    "    parser.add_argument('-cf','--checkpoint-frequency', default=20, type=int, metavar='N',\n",
    "                        help='create a checkpoint every N epochs')\n",
    "    parser.add_argument('-r', '--resume', default='', type=str, metavar='FILENAME',\n",
    "                        help='checkpoint to resume (file name)')\n",
    "    parser.add_argument('--nolog', action='store_true', help='forbiden log function')\n",
    "    parser.add_argument('--evaluate', default='h36m_best_epoch.bin', type=str, metavar='FILENAME', help='checkpoint to evaluate (file name)')\n",
    "    parser.add_argument('--render', action='store_true', help='visualize a particular video')\n",
    "    parser.add_argument('--by-subject', action='store_true', help='break down error by subject (on evaluation)')\n",
    "    parser.add_argument('--export-training-curves', action='store_true', help='save training curves as .png images')\n",
    "\n",
    "\n",
    "    # Model arguments\n",
    "    parser.add_argument('-s', '--stride', default=243, type=int, metavar='N', help='chunk size to use during training')\n",
    "    parser.add_argument('-e', '--epochs', default=400, type=int, metavar='N', help='number of training epochs')\n",
    "    parser.add_argument('-b', '--batch-size', default=11, type=int, metavar='N', help='batch size in terms of predicted frames')\n",
    "    parser.add_argument('-drop', '--dropout', default=0., type=float, metavar='P', help='dropout probability')\n",
    "    parser.add_argument('-lr', '--learning-rate', default=0.00006, type=float, metavar='LR', help='initial learning rate')\n",
    "    parser.add_argument('-lrd', '--lr-decay', default=0.993, type=float, metavar='LR', help='learning rate decay per epoch')\n",
    "    parser.add_argument('--coverlr', action='store_true', help='cover learning rate with assigned during resuming previous model')\n",
    "    parser.add_argument('-mloss', '--min_loss', default=100000, type=float, help='assign min loss(best loss) during resuming previous model')\n",
    "    parser.add_argument('-no-da', '--no-data-augmentation', dest='data_augmentation', action='store_false',\n",
    "                        help='disable train-time flipping')\n",
    "    parser.add_argument('-cs', default=512, type=int, help='channel size of model, only for trasformer') \n",
    "    parser.add_argument('-dep', default=8, type=int, help='depth of model')    \n",
    "    parser.add_argument('-alpha', default=0.01, type=float, help='used for wf_mpjpe')\n",
    "    parser.add_argument('-beta', default=2, type=float, help='used for wf_mpjpe')\n",
    "    parser.add_argument('--postrf', action='store_true', help='use the post refine module')\n",
    "    parser.add_argument('--ftpostrf', action='store_true', help='For fintune to post refine module')\n",
    "    # parser.add_argument('-no-tta', '--no-test-time-augmentation', dest='test_time_augmentation', action='store_false',\n",
    "    #                     help='disable test-time flipping')\n",
    "    # parser.add_argument('-arc', '--architecture', default='3,3,3', type=str, metavar='LAYERS', help='filter widths separated by comma')\n",
    "    parser.add_argument('-f', '--number-of-frames', default='243', type=int, metavar='N',\n",
    "                        help='how many frames used as input')\n",
    "    # parser.add_argument('--causal', action='store_true', help='use causal convolutions for real-time processing')\n",
    "    # parser.add_argument('-ch', '--channels', default=1024, type=int, metavar='N', help='number of channels in convolution layers')\n",
    "\n",
    "    # Experimental\n",
    "    parser.add_argument('-gpu', default='0', type=str, help='assign the gpu(s) to use')\n",
    "    parser.add_argument('--subset', default=1, type=float, metavar='FRACTION', help='reduce dataset size by fraction')\n",
    "    parser.add_argument('--downsample', default=1, type=int, metavar='FACTOR', help='downsample frame rate by factor (semi-supervised)')\n",
    "    parser.add_argument('--warmup', default=1, type=int, metavar='N', help='warm-up epochs for semi-supervision')\n",
    "    parser.add_argument('--no-eval', action='store_true', help='disable epoch evaluation while training (small speed-up)')\n",
    "    parser.add_argument('--dense', action='store_true', help='use dense convolutions instead of dilated convolutions')\n",
    "    parser.add_argument('--disable-optimizations', action='store_true', help='disable optimized model for single-frame predictions')\n",
    "    parser.add_argument('--linear-projection', action='store_true', help='use only linear coefficients for semi-supervised projection')\n",
    "    parser.add_argument('--no-bone-length', action='store_false', dest='bone_length_term',\n",
    "                        help='disable bone length term in semi-supervised settings')\n",
    "    parser.add_argument('--no-proj', action='store_true', help='disable projection for semi-supervised setting')\n",
    "    parser.add_argument('--ft', action='store_true', help='use ft 2d(only for detection keypoints!)')\n",
    "    parser.add_argument('--ftpath', default='checkpoint/exp13_ft2d', type=str, help='assign path of ft2d model chk path')\n",
    "    parser.add_argument('--ftchk', default='epoch_330.pth', type=str, help='assign ft2d model checkpoint file name')\n",
    "    parser.add_argument('--no_eval', action='store_true', default=False, help='no_eval')\n",
    "    \n",
    "    # Visualization\n",
    "    parser.add_argument('--viz-subject', type=str, metavar='STR', help='subject to render')\n",
    "    parser.add_argument('--viz-action', type=str, metavar='STR', help='action to render')\n",
    "    parser.add_argument('--viz-camera', type=int, default=1, metavar='N', help='camera to render')\n",
    "    parser.add_argument('--viz-video', type=str, metavar='PATH', help='path to input video')\n",
    "    parser.add_argument('--viz-skip', type=int, default=0, metavar='N', help='skip first N frames of input video')\n",
    "    parser.add_argument('--viz-output', type=str, metavar='PATH', help='output file name (.gif or .mp4)')\n",
    "    parser.add_argument('--viz-export', type=str, metavar='PATH', help='output file name for coordinates')\n",
    "    parser.add_argument('--viz-bitrate', type=int, default=3000, metavar='N', help='bitrate for mp4 videos')\n",
    "    parser.add_argument('--viz-no-ground-truth', action='store_true', help='do not show ground-truth poses')\n",
    "    parser.add_argument('--viz-limit', type=int, default=-1, metavar='N', help='only render first N frames')\n",
    "    parser.add_argument('--viz-downsample', type=int, default=1, metavar='N', help='downsample FPS by a factor N')\n",
    "    parser.add_argument('--viz-size', type=int, default=5, metavar='N', help='image size')\n",
    "    parser.add_argument('--compare', action='store_true', default=False, help='Whether to compare with other methods e.g. Poseformer')\n",
    "    # parser.add_argument('-comchk', type=str, default='/mnt/data3/home/zjl/workspace/3dpose/PoseFormer/checkpoint/detected81f.bin', help='checkpoint of comparison methods')\n",
    "\n",
    "    # ft2d.py\n",
    "    parser.add_argument('-lcs', '--linear_channel_size', type=int, default=1024, metavar='N', help='channel size of the LinearModel')\n",
    "    parser.add_argument('-depth', type=int, default=4, metavar='N', help='nums of blocks of the LinearModel')\n",
    "    parser.add_argument('-ldg', '--lr_decay_gap', type=float, default=10000, metavar='N', help='channel size of the LinearModel')\n",
    "\n",
    "    parser.add_argument('-scale', default=1.0, type=float, help='the scale of SNR')\n",
    "    parser.add_argument('-timestep', type=int, default=1000, metavar='N', help='timestep')\n",
    "    #parser.add_argument('-timestep_eval', type=int, default=1000, metavar='N', help='timestep_eval')\n",
    "    parser.add_argument('-sampling_timesteps', type=int, default=5, metavar='N', help='sampling_timesteps')\n",
    "    parser.add_argument('-num_proposals', type=int, default=5, metavar='N')\n",
    "    parser.add_argument('--debug', action='store_true', default=False, help='debugging mode')\n",
    "    parser.add_argument('--p2', action='store_true', default=False, help='using protocol #2, i.e., P-MPJPE')\n",
    "\n",
    "\n",
    "    parser.set_defaults(bone_length_term=True)\n",
    "    parser.set_defaults(data_augmentation=True)\n",
    "    #parser.set_defaults(test_time_augmentation=True)\n",
    "    parser.set_defaults(test_time_augmentation=False)\n",
    "\n",
    "    args = parser.parse_args('')\n",
    "    # Check invalid configuration\n",
    "    if args.resume and args.evaluate:\n",
    "        print('Invalid flags: --resume and --evaluate cannot be set at the same time')\n",
    "        exit()\n",
    "        \n",
    "    if args.export_training_curves and args.no_eval:\n",
    "        print('Invalid flags: --export-training-curves and --no-eval cannot be set at the same time')\n",
    "        exit()\n",
    "\n",
    "    return args\n",
    "\n",
    "d3dp_args = d3dp_parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.h36m_dataset import Human36mDataset\n",
    "dataset_path = 'data/data_3d_' + d3dp_args.dataset + '.npz'\n",
    "dataset = Human36mDataset(dataset_path)\n",
    "joints_left, joints_right = list(dataset.skeleton().joints_left()), list(dataset.skeleton().joints_right())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pos = D3DP(d3dp_args, joints_left, joints_right,  is_train=False, num_proposals=d3dp_args.num_proposals, sampling_timesteps=d3dp_args.sampling_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/h36m_best_epoch.bin\n",
      "This model was trained for 194 epochs\n"
     ]
    }
   ],
   "source": [
    "# make model parallel\n",
    "if torch.cuda.is_available():\n",
    "    #model_pos = nn.DataParallel(model_pos)\n",
    "    model_pos = model_pos.float().cuda()\n",
    "\n",
    "if d3dp_args.resume or d3dp_args.evaluate:\n",
    "    chk_filename = os.path.join(d3dp_args.checkpoint, d3dp_args.resume if d3dp_args.resume else d3dp_args.evaluate)\n",
    "    # chk_filename = args.resume or args.evaluate\n",
    "    print('Loading checkpoint', chk_filename)\n",
    "    checkpoint = torch.load(chk_filename, map_location=lambda storage, loc: storage)\n",
    "    print('This model was trained for {} epochs'.format(checkpoint['epoch']))\n",
    "    model_pos.load_state_dict(checkpoint['model_pos'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_data_prepare(receptive_field, inputs_2d, inputs_3d):\n",
    "    # inputs_2d_p = torch.squeeze(inputs_2d)\n",
    "    # inputs_3d_p = inputs_3d.permute(1,0,2,3)\n",
    "    # out_num = inputs_2d_p.shape[0] - receptive_field + 1\n",
    "    # eval_input_2d = torch.empty(out_num, receptive_field, inputs_2d_p.shape[1], inputs_2d_p.shape[2])\n",
    "    # for i in range(out_num):\n",
    "    #     eval_input_2d[i,:,:,:] = inputs_2d_p[i:i+receptive_field, :, :]\n",
    "    # return eval_input_2d, inputs_3d_p\n",
    "    ### split into (f/f1, f1, n, 2)\n",
    "    assert inputs_2d.shape[:-1] == inputs_3d.shape[:-1], \"2d and 3d inputs shape must be same! \"+str(inputs_2d.shape)+str(inputs_3d.shape)\n",
    "    inputs_2d_p = torch.squeeze(inputs_2d)\n",
    "    inputs_3d_p = torch.squeeze(inputs_3d)\n",
    "\n",
    "    if inputs_2d_p.shape[0] / receptive_field > inputs_2d_p.shape[0] // receptive_field: \n",
    "        out_num = inputs_2d_p.shape[0] // receptive_field+1\n",
    "    elif inputs_2d_p.shape[0] / receptive_field == inputs_2d_p.shape[0] // receptive_field:\n",
    "        out_num = inputs_2d_p.shape[0] // receptive_field\n",
    "\n",
    "    eval_input_2d = torch.empty(out_num, receptive_field, inputs_2d_p.shape[1], inputs_2d_p.shape[2])\n",
    "    eval_input_3d = torch.empty(out_num, receptive_field, inputs_3d_p.shape[1], inputs_3d_p.shape[2])\n",
    "\n",
    "    for i in range(out_num-1):\n",
    "        eval_input_2d[i,:,:,:] = inputs_2d_p[i*receptive_field:i*receptive_field+receptive_field,:,:]\n",
    "        eval_input_3d[i,:,:,:] = inputs_3d_p[i*receptive_field:i*receptive_field+receptive_field,:,:]\n",
    "    if inputs_2d_p.shape[0] < receptive_field:\n",
    "        from torch.nn import functional as F\n",
    "        pad_right = receptive_field-inputs_2d_p.shape[0]\n",
    "        inputs_2d_p = rearrange(inputs_2d_p, 'b f c -> f c b')\n",
    "        inputs_2d_p = F.pad(inputs_2d_p, (0,pad_right), mode='replicate')\n",
    "        # inputs_2d_p = np.pad(inputs_2d_p, ((0, receptive_field-inputs_2d_p.shape[0]), (0, 0), (0, 0)), 'edge')\n",
    "        inputs_2d_p = rearrange(inputs_2d_p, 'f c b -> b f c')\n",
    "    if inputs_3d_p.shape[0] < receptive_field:\n",
    "        pad_right = receptive_field-inputs_3d_p.shape[0]\n",
    "        inputs_3d_p = rearrange(inputs_3d_p, 'b f c -> f c b')\n",
    "        inputs_3d_p = F.pad(inputs_3d_p, (0,pad_right), mode='replicate')\n",
    "        inputs_3d_p = rearrange(inputs_3d_p, 'f c b -> b f c')\n",
    "    eval_input_2d[-1,:,:,:] = inputs_2d_p[-receptive_field:,:,:]\n",
    "    eval_input_3d[-1,:,:,:] = inputs_3d_p[-receptive_field:,:,:]\n",
    "\n",
    "    return eval_input_2d, eval_input_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 243, 17, 3])\n",
      "<class 'list'> 5 torch.Size([11, 5, 243, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.66s/it]\n"
     ]
    }
   ],
   "source": [
    "results_all = []\n",
    "model_pos.eval()       \n",
    "model_pos.device = 'cuda'\n",
    "with torch.no_grad():\n",
    "    for batch_input, batch_gt in tqdm(test_loader):\n",
    "        N, T = batch_gt.shape[:2] # B, N\n",
    "        if torch.cuda.is_available():\n",
    "            batch_input = batch_input.cuda()\n",
    "        if args.flip:    \n",
    "            batch_input_flip = flip_data(batch_input)\n",
    "            print(batch_input_flip.shape)\n",
    "            predicted_3d_pos = model_pos(batch_input.float()[:,:,:,:2], None, input_2d_flip=batch_input_flip.float()[:,:,:,:2])\n",
    "            #predicted_3d_pos_flip = model_pos(batch_input_flip)\n",
    "            #predicted_3d_pos_2 = flip_data(predicted_3d_pos_flip)                   # Flip back\n",
    "            #predicted_3d_pos = (predicted_3d_pos_1+predicted_3d_pos_2) / 2\n",
    "            print(type(predicted_3d_pos), len(predicted_3d_pos), predicted_3d_pos[0].shape)\n",
    "        else:\n",
    "            predicted_3d_pos = model_pos(batch_input)\n",
    "        for i in range(len(predicted_3d_pos)):\n",
    "            results_all.append(predicted_3d_pos[i].cpu().numpy())\n",
    "        #results_all.append(predicted_3d_pos.cpu().numpy())\n",
    "results_all = np.concatenate(results_all)\n",
    "#results_all = datareader.denormalize(results_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 5, 243, 17, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.2839991 ,  0.6319914 , -0.3044045 ],\n",
       "        [ 0.5497556 ,  0.41571048, -0.121594  ],\n",
       "        [ 0.3424947 ,  0.5514433 , -0.30137658],\n",
       "        [ 0.5646411 ,  0.64200276, -0.01160304],\n",
       "        [ 0.6602721 ,  0.51664174,  0.04166837],\n",
       "        [ 0.6816317 ,  0.29820374,  0.17094654],\n",
       "        [ 0.6440543 ,  0.04696334,  0.16382079],\n",
       "        [ 0.2817564 ,  0.65781814, -0.27978858],\n",
       "        [ 0.63170516,  0.563669  ,  0.07134522],\n",
       "        [ 0.56473154,  0.31432903,  0.08045112],\n",
       "        [ 0.63782513,  0.22986901,  0.21254227],\n",
       "        [ 0.46128896,  0.5747071 , -0.10331982],\n",
       "        [ 0.5176493 ,  0.77502596, -0.11199886],\n",
       "        [ 0.6877425 ,  0.22399183,  0.18962893],\n",
       "        [ 0.6727799 ,  0.11066622,  0.21668226],\n",
       "        [ 0.57504237,  0.25577262,  0.15488945],\n",
       "        [ 0.6294997 ,  0.51427007, -0.14901315]], dtype=float32),\n",
       " array([[ 0.57071877,  0.20632711,  0.17829446],\n",
       "        [ 0.7602687 ,  0.19235393,  0.22151288],\n",
       "        [ 0.27884093,  0.51204085, -0.3774857 ],\n",
       "        [ 0.6876565 ,  0.4271426 , -0.02148248],\n",
       "        [ 0.6013186 ,  0.3200609 ,  0.21630709],\n",
       "        [ 0.48311704,  0.7449571 , -0.10549428],\n",
       "        [ 0.7367255 ,  0.02330584,  0.20110394],\n",
       "        [ 0.3545685 ,  0.31592846, -0.27313653],\n",
       "        [ 0.2881962 ,  0.44795653, -0.27904928],\n",
       "        [ 0.45147145,  0.75051606, -0.04143269],\n",
       "        [ 0.45921206,  0.8216909 ,  0.07323863],\n",
       "        [ 0.723516  ,  0.25167018,  0.09882626],\n",
       "        [ 0.53210366,  0.5675553 ,  0.05962049],\n",
       "        [ 0.62933785,  0.2775008 ,  0.07197072],\n",
       "        [ 0.79251933,  0.03841115,  0.31584126],\n",
       "        [ 0.5251618 ,  0.40147528, -0.19637056],\n",
       "        [ 0.775292  ,  0.20524882,  0.09201236]], dtype=float32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all[0, 0, 0], results_all[3, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.       ,    0.       ,    0.       ],\n",
       "       [ 265.75647  , -216.28091  ,  182.81052  ],\n",
       "       [  58.495605 ,  -80.548096 ,    3.0279236],\n",
       "       [ 280.64203  ,   10.011353 ,  292.80148  ],\n",
       "       [ 376.273    , -115.34967  ,  346.07288  ],\n",
       "       [ 397.63263  , -333.78766  ,  475.35104  ],\n",
       "       [ 360.05524  , -585.0281   ,  468.22528  ],\n",
       "       [  -2.2426758,   25.826721 ,   24.615936 ],\n",
       "       [ 347.70605  ,  -68.32239  ,  375.74973  ],\n",
       "       [ 280.73248  , -317.66235  ,  384.85562  ],\n",
       "       [ 353.82605  , -402.12238  ,  516.9468   ],\n",
       "       [ 177.28989  ,  -57.2843   ,  201.08469  ],\n",
       "       [ 233.6502   ,  143.03455  ,  192.40565  ],\n",
       "       [ 403.7434   , -407.99957  ,  494.03345  ],\n",
       "       [ 388.78082  , -521.3252   ,  521.0868   ],\n",
       "       [ 291.04327  , -376.21878  ,  459.29395  ],\n",
       "       [ 345.5006   , -117.72131  ,  155.39136  ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rootrel_pose(results_all[0, 0, 0]*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_3d_pose([get_rootrel_pose(results_all[0, 0, 5]*1000)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motionbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "996031ba2a0f3c1298a339c0299835a7fe1ef636d9e79358bc474ca43ed2ac18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
